{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2753e4-9dc7-41d7-b080-d1abfbb16b28",
   "metadata": {},
   "source": [
    "# Laboratorio de regresión - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396ea70-b88c-4346-a14f-4aa7ca2e1b00",
   "metadata": {},
   "source": [
    "|                |   |\r\n",
    ":----------------|---|\r\n",
    "| **Nombre**     |  Regina Tamayo León |\r\n",
    "| **Fecha**      | 8 feb 2026  |\r\n",
    "| **Expediente** |757857   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77def53e-10bf-474e-acdf-728e07bef102",
   "metadata": {},
   "source": [
    "## Modelos penalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb791d-1843-4b4d-bd8b-69e6419511e8",
   "metadata": {},
   "source": [
    "Hasta ahora la función de costo que usamos para decidir qué tan bueno es nuestro modelo al momento de ajustar es:\n",
    "\n",
    "$$ \\text{RSS} = \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6b5e6b-abe9-4b75-b045-e4444de4fc35",
   "metadata": {},
   "source": [
    "Dado que los errores obtenidos son una combinación de sesgo y varianza, puede ser que se sesgue un parámetro para minimizar el error. Esto significa que el modelo puede decidir que la salida no sea una combinación de los factores, sino una fuerte predilección sobre uno de los factores solamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84901f9e-5551-455a-a70c-c7e39d9e55ae",
   "metadata": {},
   "source": [
    "E.g. se quiere ajustar un modelo\n",
    "\n",
    "$$ \\hat{z} = \\hat{\\beta_0} + \\hat{\\beta_1} x + \\hat{\\beta_2} y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f473fc-6364-4b15-9bd4-f21a94cae151",
   "metadata": {},
   "source": [
    "Se ajusta el modelo y se decide que la mejor decisión es $\\hat{\\beta_1} = 10000$ y $\\hat{\\beta_2}=50$. Considera limitaciones de problemas reales:\n",
    "- Quizás los parámetros son ajustes de maquinaria que se deben realizar para conseguir el mejor producto posible, y que $10000$ sea imposible de asignar.\n",
    "- Quizás los datos actuales están sesgados y sólo hacen parecer que uno de los factores importa más que el otro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff32fbaa-7965-42c1-9b73-3640414b77f2",
   "metadata": {},
   "source": [
    "Una de las formas en las que se puede mitigar este problema es penalizando a los parámetros del modelo, cambiando la función de costo:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc78736-9d8e-4e8b-94f3-6647bdaeb0d1",
   "metadata": {},
   "source": [
    "$$ \\text{RSS}_{L2} = \\sum_{i=1}^n e_i^2  + \\lambda \\sum_{j=1}^p \\hat{\\beta_j}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d942bf5-fb39-44a0-b612-4ec05ab99b71",
   "metadata": {},
   "source": [
    "El *L2* significa que se está agregando una penalización de segundo orden. Lo que hace esta penalización es que los factores ahora sólo tendrán permitido crecer si hay una reducción al menos proporcional en el error (sacrificamos sesgo, pero reducimos la varianza)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0cafb-c152-48e4-a345-bb5348eb16c7",
   "metadata": {},
   "source": [
    "Asimismo, existe la penalización *L1*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f2d93-d151-47ed-834f-4a7e91e94286",
   "metadata": {},
   "source": [
    "$$ \\text{RSS}_{L1} = \\sum_{i=1}^n e_i^2  + \\lambda \\sum_{j=1}^p |\\hat{\\beta_j}| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95f232-25ab-4b4c-99b3-075a18878d95",
   "metadata": {},
   "source": [
    "A las penalizaciones *L2* y *L1* se les conoce también como Ridge y Lasso, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41dfafb-fd1f-475a-a718-dec1e3773326",
   "metadata": {},
   "source": [
    "Para realizar una regresión con penalización de Ridge o de Lasso usamos el objeto `Ridge(alpha=?)` o `Lasso(alpha=?)` en lugar de `LinearRegression()` de `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36cb41-005e-4cb0-b8ab-92e6ae6c4c19",
   "metadata": {},
   "source": [
    "Utiliza el dataset de publicidad (Advertising.csv) y realiza 3 regresiones múltiples:\n",
    "\n",
    "$$ \\text{sales} = \\beta_0 + \\beta_1 (\\text{TV}) + \\beta_2 (\\text{radio}) + \\beta_3 (\\text{newspaper}) + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92e4008-ada1-4e82-9756-23fd357821d2",
   "metadata": {},
   "source": [
    "1. Sin penalización\n",
    "2. Con penalización L2\n",
    "3. Con penalización L1\n",
    "\n",
    "¿Qué puedes observar al ajustar los valores de `alpha`? \n",
    "\n",
    "Compara los resultados de los coeficientes utilizando valores diferentes de $\\alpha$ y los $R^2$ resultantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a895a79-6ed8-4e00-9e6e-7a3570d12376",
   "metadata": {},
   "source": [
    "**Sin penalización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489b6c52-9025-4ca7-8e96-ca70c294cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 1: Sin Penalización\n",
      "beta_0 (Intercepto): 2.938889369459403\n",
      "Coeficientes (beta_1, beta_2, beta_3): [ 0.04576465  0.18853002 -0.00103749]\n",
      "R^2 score: 0.8972106381789522\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv('Advertising.csv', index_col=0)\n",
    "\n",
    "X = np.array(df[['TV', 'radio', 'newspaper']])\n",
    "y = np.array(df['sales'])\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "\n",
    "print(\"Modelo 1: Sin Penalización\")\n",
    "print(\"beta_0 (Intercepto):\", lr.intercept_)\n",
    "print(\"Coeficientes (beta_1, beta_2, beta_3):\", lr.coef_)\n",
    "print(\"R^2 score:\", lr.score(X, y))\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37c112-6416-4b19-b66e-566e0388ad2b",
   "metadata": {},
   "source": [
    "**Con penalización L2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53efacc6-e600-4705-b180-3d297324a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 2: Ridge (L2)\n",
      "beta_0: 2.9466816422932336\n",
      "Coeficientes: [ 0.04576446  0.18803935 -0.00091803]\n",
      "R^2 score: 0.8972089327944492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#alpha=100\n",
    "ridge = Ridge(alpha=100)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "print(\"Modelo 2: Ridge (L2)\")\n",
    "print(\"beta_0:\", ridge.intercept_)\n",
    "print(\"Coeficientes:\", ridge.coef_)\n",
    "print(\"R^2 score:\", ridge.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ff5b5e-4833-4310-8559-651d42ce3cdf",
   "metadata": {},
   "source": [
    "los coeficientes son casi iguales a la regresión lineal, pero ligeramente más contraídos (especialmente el de Radio bajó de 0.1885 a 0.1880)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da74a1-1133-4f1e-9376-3e357b8eb664",
   "metadata": {},
   "source": [
    "**Con penalización L1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4148eb02-2674-4d66-912d-d0040d6b7e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo 3: Lasso (L1)\n",
      "beta_0: 3.040215583480375\n",
      "Coeficientes: [0.04566142 0.1834644  0.        ]\n",
      "R^2 score: 0.8970235728389688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "#alpha=1.0\n",
    "lasso = Lasso(alpha=1.0)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(\"Modelo 3: Lasso (L1)\")\n",
    "print(\"beta_0:\", lasso.intercept_)\n",
    "print(\"Coeficientes:\", lasso.coef_)\n",
    "print(\"R^2 score:\", lasso.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4546ec3b-1b45-4be2-ac2a-9c1af99e3cde",
   "metadata": {},
   "source": [
    "el tercer coeficiente (Newspaper) se vuelve 0.0. Lasso decidió que invertir en periódico no tiene un impacto real en las ventas cuando ya existe TV y Radio en el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cd848-efa6-4d5d-800a-1ee0ebac1aef",
   "metadata": {},
   "source": [
    "**¿Qué puedes observar al ajustar los valores de alpha?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6027-870a-4f5e-8359-4c0162f77c35",
   "metadata": {},
   "source": [
    "En Ridge (L2): Los coeficientes se reducen gradualmente pero nunca mueren del todo. El R^2 se mantiene muy estable. \n",
    "En Lasso (L1): Ayuda a la selección de variables. Si se sube mucho el alpha, variables como newspaper desaparecen del modelo (coeficiente 0), simplificándolo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89951c7-fe9c-45a7-b76f-4cf1cee468bb",
   "metadata": {},
   "source": [
    "Ridge (L2): A medida que aumentas alpha, los coeficientes se reducen gradualmente hacia cero, pero es muy difícil que lleguen a ser exactamente cero. Por ejemplo, con alpha=1000, el coeficiente de newspaper cambió de signo y se acercó a cero, pero sigue presente.Lasso (L1): Es mucho más drástico. Con un alpha pequeño (como 1.0), Lasso eliminó por completo la variable newspaper igualando su coeficiente a 0.0. Esto demuestra que Lasso funciona como un método de selección de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d520cb5a-edd4-4a00-a0c8-5866d080a796",
   "metadata": {},
   "source": [
    "El valor de R^2 disminuye ligeramente a medida que aumenta la penalización. Esto sucede porque estamos agregando sesgo al modelo a propósito para reducir su varianza.\n",
    "La caída en el R^2 es mínima (pasa de 0.8972 a 0.8970), lo que indica que podemos tener un modelo más simple (con menos variables en el caso de Lasso) sin perder prácticamente nada de capacidad predictiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26d87-faa6-4014-aba3-752f3c3dbd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
